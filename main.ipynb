{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# EMNIST-GAN\n",
    "so, I once answered a lecturer (Mr Percy Wong) to train a 5 year old how to recognize digits, I will use a GAN.\n",
    "\n",
    "so let's use a GAN.\n",
    "\n",
    "In this notebook, I will be making use of the EMNIST (Extended Modified NIST) dataset of handwritten digits AND alphabets to train a Generator and Discriminator. The use-case for this would be to solve captchas but primarily to settle that statement that I said about GANs a long time ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ------ DANGER ZONE -------\n",
    "## DO NOT RUN THE NEXT CELL IF YOU DON'T INTEND TO PIP INSTALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "Requirement already satisfied: extra_keras_datasets in /home/lucas/.local/lib/python3.9/site-packages (1.2.0)\r\n",
      "Requirement already satisfied: scikit-learn in /home/lucas/.local/lib/python3.9/site-packages (from extra_keras_datasets) (0.24.1)\r\n",
      "Requirement already satisfied: numpy in /usr/lib/python3.9/site-packages (from extra_keras_datasets) (1.20.0)\r\n",
      "Requirement already satisfied: scipy in /home/lucas/.local/lib/python3.9/site-packages (from extra_keras_datasets) (1.6.0)\r\n",
      "Requirement already satisfied: pandas in /home/lucas/.local/lib/python3.9/site-packages (from extra_keras_datasets) (1.2.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3.9/site-packages (from pandas->extra_keras_datasets) (2.8.1)\r\n",
      "Requirement already satisfied: numpy in /usr/lib/python3.9/site-packages (from extra_keras_datasets) (1.20.0)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/lucas/.local/lib/python3.9/site-packages (from pandas->extra_keras_datasets) (2020.5)\r\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->extra_keras_datasets) (1.15.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/lib/python3.9/site-packages (from scikit-learn->extra_keras_datasets) (1.0.0)\r\n",
      "Requirement already satisfied: numpy in /usr/lib/python3.9/site-packages (from extra_keras_datasets) (1.20.0)\r\n",
      "Requirement already satisfied: scipy in /home/lucas/.local/lib/python3.9/site-packages (from extra_keras_datasets) (1.6.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/lucas/.local/lib/python3.9/site-packages (from scikit-learn->extra_keras_datasets) (2.1.0)\r\n",
      "Requirement already satisfied: numpy in /usr/lib/python3.9/site-packages (from extra_keras_datasets) (1.20.0)\r\n",
      "^C\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/bin/pip3\", line 33, in <module>\r\n",
      "    sys.exit(load_entry_point('pip==20.3.1', 'console_scripts', 'pip3')())\r\n",
      "  File \"/usr/lib/python3.9/site-packages/pip/_internal/cli/main.py\", line 75, in main\r\n",
      "    return command.main(cmd_args)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 117, in main\r\n",
      "    return self._main(args)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/pip/_internal/cli/base_command.py\", line 261, in _main\r\n",
      "    self.handle_pip_version_check(options)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/pip/_internal/cli/req_command.py\", line 153, in handle_pip_version_check\r\n",
      "    pip_self_version_check(session, options)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/pip/_internal/self_outdated_check.py\", line 162, in pip_self_version_check\r\n",
      "    best_candidate = finder.find_best_candidate(\"pip\").best_candidate\r\n",
      "  File \"/usr/lib/python3.9/site-packages/pip/_internal/index/package_finder.py\", line 882, in find_best_candidate\r\n",
      "    candidates = self.find_all_candidates(project_name)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/pip/_internal/index/package_finder.py\", line 825, in find_all_candidates\r\n",
      "    package_links = self.process_project_url(\r\n",
      "  File \"/usr/lib/python3.9/site-packages/pip/_internal/index/package_finder.py\", line 796, in process_project_url\r\n",
      "    package_links = self.evaluate_links(\r\n",
      "  File \"/usr/lib/python3.9/site-packages/pip/_internal/index/package_finder.py\", line 778, in evaluate_links\r\n",
      "    candidate = self.get_install_candidate(link_evaluator, link)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/pip/_internal/index/package_finder.py\", line 763, in get_install_candidate\r\n",
      "    return InstallationCandidate(\r\n",
      "  File \"/usr/lib/python3.9/site-packages/pip/_internal/models/candidate.py\", line 21, in __init__\r\n",
      "    self.version = parse_version(version)  # type: _BaseVersion\r\n",
      "  File \"/usr/lib/python3.9/site-packages/packaging/version.py\", line 57, in parse\r\n",
      "    return Version(version)\r\n",
      "  File \"/usr/lib/python3.9/site-packages/packaging/version.py\", line 301, in __init__\r\n",
      "    self._version = _Version(\r\n",
      "  File \"<string>\", line 1, in <lambda>\r\n",
      "KeyboardInterrupt\r\n",
      "Defaulting to user installation because normal site-packages is not writeable\r\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install extra_keras_datasets\n",
    "!pip3 install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'h5py.h5a.AttrID' has no attribute '__reduce_cython__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a1bfb085adef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mextra_keras_datasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0memnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2DTranspose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglorot_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRandomNormal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38/lib/python3.8/site-packages/extra_keras_datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'v1.2.0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0memnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msvhn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38/lib/python3.8/site-packages/extra_keras_datasets/emnist.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \"\"\"\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38/lib/python3.8/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column_lib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnode_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtraining_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork_serialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss_scale_optimizer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlso\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmixed_precision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m   \u001b[0mHDF5_OBJECT_HEADER_LIMIT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64512\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/py38/lib/python3.8/site-packages/h5py/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# --- Public API --------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5fd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5z\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5pl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_hl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/h5a.pyx\u001b[0m in \u001b[0;36minit h5py.h5a\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'h5py.h5a.AttrID' has no attribute '__reduce_cython__'"
     ]
    }
   ],
   "source": [
    "from extra_keras_datasets import emnist\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout, LeakyReLU, Conv2D, Flatten, BatchNormalization, MaxPooling2D, Activation, Lambda, Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.initializers import glorot_normal, RandomNormal\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import random\n",
    "from tensorflow.keras import mixed_precision\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras import backend\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Download the dataset byclass for EMNIST, change the dataset and labels here if you wish (there is EMNIST for japanese characters too!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = emnist.load_data(type='byclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Dataset classes\n",
    "These are the labels of our dataset, we'll use this to convert it from the result to the actual human-readable class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_idx = ['0','1','2','3','4','5','6','7','8','9',\n",
    "             'A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z',\n",
    "             'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load samples\n",
    "Let's take some samples from our dataset and take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(25):\n",
    "    # pick random image index from dataset to show\n",
    "    index = random.randint(0, len(x_train))\n",
    "    # define subplot\n",
    "    plt.subplot(5, 5, 1 + i)\n",
    "    plt.tight_layout()\n",
    "    # turn off axis\n",
    "    plt.axis('off')\n",
    "    # change to grayscale as our images are in grayscale\n",
    "    plt.imshow(x_train[index], cmap='gray')\n",
    "    # show image title according to the real dataset classes\n",
    "    plt.title(class_idx[y_train[index]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Reshape the data for training\n",
    "Reshape x to 28,28,1 to include the colour channel and perform one hot encoding on y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# add an additional column for channels (our images are grayscale, so 1) and convert to float32\n",
    "# expand to 3d, e.g. add channels\n",
    "X = np.expand_dims(x_train, axis=-1).astype(\"float16\")\n",
    "# convert from ints to floats\n",
    "X = X.astype('float16')\n",
    "# scale from [0,255] to [-1,1]\n",
    "X = (X - 127.5) / 127.5\n",
    "dataset = [X, y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create the discriminator network\n",
    "I will be using SimpleNet for its simplicity and high accuracy.\n",
    "GANs take forever to train, every bit of performance is welcome. SimpleNet is also impressively accurate\n",
    "\n",
    "model I built below is based off the pytorch model with LeakyReLU\n",
    "\n",
    "SimpleNet paper : https://arxiv.org/abs/1608.06037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# custom activation function\n",
    "def custom_activation(output):\n",
    "\tlogexpsum = backend.sum(backend.exp(output), axis=-1, keepdims=True)\n",
    "\tresult = logexpsum / (logexpsum + 1.0)\n",
    "\treturn result\n",
    "\n",
    "model = Sequential()\n",
    "# Block 1\n",
    "model.add(Conv2D(64, (3,3), padding='same', kernel_initializer=glorot_normal()))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "# Block 2,3,4 (repeated)\n",
    "for i in range(3):\n",
    "    model.add(Conv2D(128, (3,3), padding=\"same\", kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "# max pooling (change strides as you wish)\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block 5,6\n",
    "for i in range (2):\n",
    "    model.add(Conv2D(128, (3,3), padding=\"same\", kernel_initializer=RandomNormal(stddev=0.01)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "# Block 7\n",
    "model.add(Conv2D(256, (3,3), padding=\"same\", kernel_initializer=glorot_normal()))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "# Another max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "# Block 8,9\n",
    "for i in range(2):\n",
    "    model.add(Conv2D(256, (3,3), padding=\"same\", kernel_initializer=glorot_normal()))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "\n",
    "# Block 10\n",
    "model.add(Conv2D(512, (3,3), padding=\"same\", kernel_initializer=glorot_normal()))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "# Block 11\n",
    "model.add(Conv2D(2048, (1,1), padding=\"same\", kernel_initializer=glorot_normal()))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "\n",
    "# Block 12\n",
    "model.add(Conv2D(256, (1,1), padding=\"same\", kernel_initializer=glorot_normal()))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "# Another another another max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block 13\n",
    "model.add(Conv2D(256, (3,3), padding=\"same\", kernel_initializer=glorot_normal()))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LeakyReLU())\n",
    "# Another another another max pooling\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(class_idx)))\n",
    "img = Input(shape=(28,28,1))\n",
    "# Unsupervised output\n",
    "features = model(img)\n",
    "# use the custom activation function from the OpenAI paper\n",
    "discriminator_out_layer = Lambda(custom_activation)(features)\n",
    "discriminator_model = Model(img, discriminator_out_layer)\n",
    "discriminator_model.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "# supervised output\n",
    "categorical_out_layer = Activation(\"softmax\")(features)\n",
    "categorical_model = Model(img, categorical_out_layer)\n",
    "categorical_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(lr=0.0002, beta_1=0.5))\n",
    "\n",
    "# plot both the classifier and discriminator models\n",
    "plot_model(categorical_model, show_shapes=True, show_layer_names=True, to_file=\"simplenet-SGAN-categorical_model.png\")\n",
    "plot_model(discriminator_model, show_shapes=True, show_layer_names=True, to_file=\"simplenet-SGAN-discriminator_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the generator model\n",
    "This will be the GAN part of the equation. I will define a simple generator network as I don't really have much experience with more advanced networks like StyleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "generator_model = Sequential()\n",
    "generator_model.add(Dense(128*7*7, input_dim=100))\n",
    "generator_model.add(LeakyReLU())\n",
    "generator_model.add(Reshape((7,7,128)))\n",
    "generator_model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding=\"same\"))\n",
    "generator_model.add(LeakyReLU())\n",
    "generator_model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding=\"same\"))\n",
    "generator_model.add(LeakyReLU())\n",
    "generator_model.add(Conv2D(1, (7,7), activation='tanh', padding=\"same\"))\n",
    "plot_model(generator_model, show_shapes=True, show_layer_names=True, to_file=\"generator-model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Generative Adversarial Network\n",
    "We need to connect the output of the generator with the discriminator, so they can train each other. Eventually the two networks will get good at:\n",
    "1. Discriminating between a real and fake number\n",
    "2. Generating data samples.\n",
    "And this is the definition of a GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# make the discriminator not trainable\n",
    "discriminator_model.trainable = False\n",
    "gan_output = discriminator_model(generator_model.output)\n",
    "gan = Model(generator_model.input, gan_output)\n",
    "# compile model\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=Adam(lr=0.002, beta_1=0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for getting real data samples\n",
    "Adapted from reference https://machinelearningmastery.com/semi-supervised-generative-adversarial-network/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# select a supervised subset of the dataset, ensures classes are balanced\n",
    "def select_supervised_samples(dataset, n_samples=100, n_classes=10):\n",
    "\tX, y = dataset\n",
    "\tX_list, y_list = list(), list()\n",
    "\tn_per_class = int(n_samples / n_classes)\n",
    "\tfor i in range(n_classes):\n",
    "\t\t# get all images for this class\n",
    "\t\tX_with_class = X[y == i]\n",
    "\t\t# choose random instances\n",
    "\t\tix = np.random.randint(0, len(X_with_class), n_per_class)\n",
    "\t\t# add to list\n",
    "\t\t[X_list.append(X_with_class[j]) for j in ix]\n",
    "\t\t[y_list.append(i) for j in ix]\n",
    "\treturn np.asarray(X_list), np.asarray(y_list)\n",
    "\n",
    "\n",
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "\t# split into images and labels\n",
    "\timages, labels = dataset\n",
    "\t# choose random instances\n",
    "\tix = np.random.randint(0, images.shape[0], n_samples)\n",
    "\t# select images and labels\n",
    "\tX, labels = images[ix], labels[ix]\n",
    "\t# generate class labels\n",
    "\ty = np.ones((n_samples, 1))\n",
    "\treturn [X, labels], y.astype(\"float32\")\n",
    "\n",
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "\t# generate points in the latent space\n",
    "\tz_input = np.random.randn(latent_dim * n_samples)\n",
    "\t# reshape into a batch of inputs for the network\n",
    "\tz_input = z_input.reshape(n_samples, latent_dim)\n",
    "\treturn z_input.astype(\"float32\")\n",
    "\n",
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "\t# generate points in latent space\n",
    "\tz_input = generate_latent_points(latent_dim, n_samples)\n",
    "\t# predict outputs\n",
    "\timages = generator.predict(z_input)\n",
    "\t# create class labels\n",
    "\ty = np.zeros((n_samples, 1))\n",
    "\treturn images, y.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def summarize_performance(step, g_model, c_model, latent_dim, dataset, n_samples=100):\n",
    "\t# prepare fake examples\n",
    "\tX, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "\t# scale from [-1,1] to [0,1]\n",
    "\tX = (X + 1) / 2.0\n",
    "\t# plot images\n",
    "\tfor i in range(100):\n",
    "\t\t# define subplot\n",
    "\t\tplt.subplot(10, 10, 1 + i)\n",
    "\t\t# turn off axis\n",
    "\t\tplt.axis('off')\n",
    "\t\t# plot raw pixel data\n",
    "\t\tplt.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "\t# save plot to file\n",
    "\tfilename1 = 'generated_plot_%04d.png' % (step+1)\n",
    "\tplt.savefig(filename1)\n",
    "\tplt.close()\n",
    "\t# evaluate the classifier model\n",
    "\tX, y = dataset\n",
    "\t_, acc = c_model.evaluate(X, y, verbose=0)\n",
    "\tprint('Classifier Accuracy: %.3f%%' % (acc * 100))\n",
    "\t# save the generator model\n",
    "\tfilename2 = 'g_model_%04d.h5' % (step+1)\n",
    "\tg_model.save(filename2)\n",
    "\t# save the classifier model\n",
    "\tfilename3 = 'c_model_%04d.h5' % (step+1)\n",
    "\tc_model.save(filename3)\n",
    "\tprint('>Saved: %s, %s, and %s' % (filename1, filename2, filename3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Adapted script from reference https://machinelearningmastery.com/semi-supervised-generative-adversarial-network/. We need a custom script for training as there is no .fit equivalent for GANs (yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "n_batch = 32\n",
    "latent_dim = 100\n",
    "# use fp16 for very fast training\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "# select supervised dataset\n",
    "X_sup, y_sup = select_supervised_samples(dataset)\n",
    "print(X_sup.shape, y_sup.shape)\n",
    "# calculate the number of batches per training epoch\n",
    "bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "# calculate the number of training iterations\n",
    "n_steps = bat_per_epo * n_epochs\n",
    "# calculate the size of half a batch of samples\n",
    "half_batch = int(n_batch / 2)\n",
    "print('n_epochs=%d, n_batch=%d, 1/2=%d, b/e=%d, steps=%d' % (n_epochs, n_batch, half_batch, bat_per_epo, n_steps))\n",
    "# manually enumerate epochs\n",
    "for i in range(n_steps):\n",
    "    # update supervised discriminator (c)\n",
    "    [Xsup_real, ysup_real], _ = generate_real_samples([X_sup, y_sup], half_batch)\n",
    "    c_loss = categorical_model.train_on_batch(Xsup_real, ysup_real)\n",
    "    # update unsupervised discriminator (d)\n",
    "    [X_real, _], y_real = generate_real_samples(dataset, half_batch)\n",
    "    d_loss1 = discriminator_model.train_on_batch(X_real, y_real)\n",
    "    X_fake, y_fake = generate_fake_samples(generator_model, latent_dim, half_batch)\n",
    "    d_loss2 = discriminator_model.train_on_batch(X_fake, y_fake)\n",
    "    # update generator (g)\n",
    "    X_gan, y_gan = generate_latent_points(latent_dim, n_batch), np.ones((n_batch, 1))\n",
    "    g_loss = gan.train_on_batch(X_gan, y_gan)\n",
    "    # summarize loss on this batch\n",
    "    sys.stdout.write('\\r'+f\"iteration{i} categorical loss: {c_loss} discriminator real loss: {d_loss1} discriminator fake loss: {d_loss2} generator loss: {g_loss}\")\n",
    "    #print(, end=\"\\r\", flush=True)\n",
    "    # evaluate the model performance every so often\n",
    "    if (i+1) % (bat_per_epo * 1) == 0:\n",
    "        summarize_performance(i, generator_model, categorical_model, latent_dim, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluate the model's performance\n",
    "not tested, training took too long to ever get to this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "score = categorical_model.evaluate(x_test, y_test)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "categorical_model.save(\"aaaaav7.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (heart-disease)",
   "language": "python",
   "name": "pycharm-c3cbfdc6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
